{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8187519-26d5-45fd-8294-a688208154c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package is available from local environement\n",
      "No broken requirements found.\n"
     ]
    }
   ],
   "source": [
    "#test if has specific package from environement\n",
    "try:\n",
    "    import pdfquery\n",
    "    print('package is available from local environement')\n",
    "except ImportError:\n",
    "    print('package of local environement is not available')\n",
    "\n",
    "!pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19823379-d33b-4691-813b-48dbc7dbf629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dancinoman/code/dancinoman/legal-explanation/le-package/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#from langchain_community.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import llm\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "from pdfquery import PDFQuery\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a506ed73-cbd3-49d2-b92f-9c1b1e9bbd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalExpert:\n",
    "    def __init__(self):\n",
    "\n",
    "\n",
    "\n",
    "        # falcon model\n",
    "        model_name = \"tiiuae/falcon-11B\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.falcon_llm = pipeline(\"text-generation\",\n",
    "                                   model=model_name,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   torch_dtype=torch.float16,\n",
    "                                   trust_remote_code=True,\n",
    "                                   device_map=\"auto\")\n",
    "\n",
    "        print(f'Model {model_name} is set.')\n",
    "        # create llm pipeline for model\n",
    "        #model_name = \"google/flan-t5-xl\"\n",
    "\n",
    "        #self.huggingface_llm = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer)\n",
    "        #print('Hugging face pipeline set.')\n",
    "        #self.openai_gpt4_llm = ChatOpenAI(temperature=0, max_tokens=256)\n",
    "        #self.chat = ChatAnthropic()\n",
    "\n",
    "\n",
    "\n",
    "        #self.chain = llm.LLMChain(llm=self.huggingface_llm, prompt=full_prompt_template)\n",
    "\n",
    "    def get_system_prompt(self, language, context):\n",
    "        system_prompt = \"\"\"\n",
    "        You are a Canadian Legal Expert.\n",
    "        Under no circumstances do you give legal advice.\n",
    "\n",
    "        You are adept at explaining the law in laymans terms, and you are able to provide context to legal questions.\n",
    "        While you can add context outside of the provided context, please do not add any information that is not directly relevant to the question, or the provided context.\n",
    "        You speak {language}.\n",
    "        ### CONTEXT\n",
    "        {context}\n",
    "        ### END OF CONTEXT\n",
    "        \"\"\"\n",
    "\n",
    "        return SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "\n",
    "    def run_chain(self, language, context, question):\n",
    "\n",
    "        self.system_prompt = self.get_system_prompt(language, context)\n",
    "        self.user_prompt = HumanMessagePromptTemplate.from_template('{question}')\n",
    "        \n",
    "        self.full_prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [self.system_prompt, self.user_prompt]\n",
    "        )\n",
    "\n",
    "        self.chain = self.full_prompt_template | self.falcon_llm\n",
    "\n",
    "        return self.chain.invoke(input={'context':context, 'language':language,'question':question})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2658d476-e093-47e0-992c-487a70e53398",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_loc = \"Legal documentation/Contract_of_PurchaseSale.pdf\"\n",
    "\n",
    "def retrieve_pdf_text(pdf_file_loc):\n",
    "\n",
    "    pdf_file = PDFQuery(pdf_file_loc)\n",
    "    pdf_file.load()\n",
    "    text_elements = pdf_file.pq('LTTextLineHorizontal')\n",
    "    return \"\".join([t.text for t in text_elements if t.text.strip() != ''])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94bc23db-6ae0-4f80-a355-21d87149f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Document Explainer (that does not give advice)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.17s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tiiuae/falcon-11B is set.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1.French\n",
      "2.English\n",
      " 2\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported message type: <class 'langchain_core.prompts.prompt.PromptTemplate'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat is about the document?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# run the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m legal_response \u001b[38;5;241m=\u001b[39m \u001b[43mmachine_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretrieve_pdf_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file_loc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Output the answer\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegal_response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlegal_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36mLegalExpert.run_chain\u001b[0;34m(self, language, context, question)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_system_prompt(language, context)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_prompt_template \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_prompt\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_prompt_template \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfalcon_llm\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m:context, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m:language,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m:question})\n",
      "File \u001b[0;32m~/code/dancinoman/legal-explanation/le-package/lib/python3.10/site-packages/langchain_core/prompts/chat.py:1200\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[0;34m(cls, messages, template_format)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   1162\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m   1163\u001b[0m     template_format: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmustache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1164\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m        a chat prompt template.\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/dancinoman/legal-explanation/le-package/lib/python3.10/site-packages/langchain_core/prompts/chat.py:996\u001b[0m, in \u001b[0;36mChatPromptTemplate.__init__\u001b[0;34m(self, messages, template_format, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    944\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    948\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \n\u001b[1;32m    951\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m \n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    997\u001b[0m         _convert_to_message(message, template_format) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m    998\u001b[0m     ]\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/code/dancinoman/legal-explanation/le-package/lib/python3.10/site-packages/langchain_core/prompts/chat.py:997\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    944\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    948\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \n\u001b[1;32m    951\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m \n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    996\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 997\u001b[0m         \u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m    998\u001b[0m     ]\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/code/dancinoman/legal-explanation/le-package/lib/python3.10/site-packages/langchain_core/prompts/chat.py:1479\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message, template_format)\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1478\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _message\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsupported message type: <class 'langchain_core.prompts.prompt.PromptTemplate'>"
     ]
    }
   ],
   "source": [
    "# create a streamlit app\n",
    "print(\"Starting Document Explainer (that does not give advice)\")\n",
    "\n",
    "machine_reader = LegalExpert()\n",
    "\n",
    "# create a upload file widget for a pdf\n",
    "\n",
    "\n",
    "language = input(\"1.French\\n2.English\\n\")\n",
    "#question = input(\"Ask a question? \")\n",
    "question = 'what is about the document?'\n",
    "\n",
    "# run the model\n",
    "legal_response = machine_reader.run_chain(\n",
    "    language=language, context=retrieve_pdf_text(pdf_file_loc), question=question\n",
    ")\n",
    "#Output the answer\n",
    "print(f\"legal_response: {legal_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7468149-aff2-44c0-8976-ad7d0aa376db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "le-package",
   "language": "python",
   "name": "le-package"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
